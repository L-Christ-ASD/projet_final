name: CI/CD --> preProduction (Deploy to AWS)

on:
  push:
    branches:
      - main
  workflow_dispatch:
jobs:
  deploy:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout du code
      uses: actions/checkout@v4

    - name: Mettre à jour les paquets
      run: |
        sudo apt-mark hold firefox snapd
        sudo apt update
        sudo apt upgrade -y
# sudo apt-mark hold firefox snapd  -->  empêche ces paquets d’être mis à jour (et donc d’appeler Snap).
    - name: Installer Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: latest

    - name: Vérifier l'installation de terraform
      run: |
        terraform --version

    - name: installer AWS CLI via pip
      run: |
        sudo apt install -y python3-pip
        pip3 install awscli --upgrade --user

    - name: Installer Ansible via PPA (dernière version stable)
      run: |
        sudo apt install -y software-properties-common
        sudo add-apt-repository --yes --update ppa:ansible/ansible
        sudo apt install -y ansible

    - name: Vérifier l'installation d'Ansible et aws
      run: |
        ansible --version
        aws --version

    - name: Configurer AWS CLI
      run: |
        aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws configure set region ${{ vars.AWS_DEFAULT_REGION }}

    - name: Vérifier l'authentification AWS
      run: |
        aws sts get-caller-identity

    - name: Valider le format Terraform
      run: terraform fmt -check
      working-directory: ./preProd-terraform

    - name: Lister les clés existantes 
      run: aws ec2 describe-key-pairs --query 'KeyPairs[*].KeyName'

    - name: Supprimer les clés existantes 
      run: |
        aws ec2 delete-key-pair --key-name "vockey" --region us-east-1
        aws ec2 delete-key-pair --key-name "vockeyprod" --region us-east-1

    - name: Lister les clés existantes 
      run: aws ec2 describe-key-pairs --query 'KeyPairs[*].KeyName'

    - name: Lister les groupes de sécurité sur AWS
      run: aws ec2 describe-security-groups --query "SecurityGroups[*].{Name:GroupName, ID:GroupId}"
##
  #  #- pour tester rke2, on commente preprod
  #    
  #  - name: Initialiser Terraform
  #    run: terraform init
  #    working-directory: ./preProd-terraform
#
#
#
  #  - name: Valider la configuration Terraform
  #    run: terraform validate
  #    working-directory: ./preProd-terraform
#
  #  - name: Initialiser Terraform
  #    run: terraform init
  #    working-directory: ./preProd-terraform
#
  #  - name: Plan Terraform
  #    run: terraform plan -no-color
  #    working-directory: ./preProd-terraform
#
  #  - name: Lancer Terraform apply
  #    run: terraform apply -auto-approve
  #    working-directory: ./preProd-terraform
#
  #  - name: Sauvegarder la clé SSH
  #    uses: actions/upload-artifact@v4
  #    with:
  #      name: vockey.pem
  #      path: ./preProd-terraform/vockey.pem
#
  #  - name: Contenu du dossier ansible
  #    run: |
  #      ls ./ansible/
#
  #  - name: Contenu du fichier inventory
  #    run: |
  #      cat ./ansible/inventory
#
  #  - name: Autoriser SSH depuis n'importe où (temporaire)
  #    run: |
  #      aws ec2 authorize-security-group-ingress --group-name admin_ssh_preprod \
  #        --protocol tcp --port 22 --cidr 0.0.0.0/0 --region us-east-1
#
  #  - name: Contenu du dossier terraform et arbo
  #    run: |
  #      pwd
  #      ls
  #      ls ./preProd-terraform/
  #      ls -l ./preProd-terraform/vockey.pem
  #      ls -R
#
  #  - name: Chmod 400 sur .pem
  #    run: |
  #      chmod 400 ./preProd-terraform/vockey.pem
  #  
  #  - name: Contenu du dossier terraform
  #    run: |
  #      ls -l ./preProd-terraform/vockey.pem
#
  #  - name: Attendre le demarrage complet du server 120s
  #    run: |
  #      sleep 120s

      # Vérifier la connexion SSH avec Ansible (ping sur toutes les machines)
#    - name: Tester la connection SSH  avec Ansible
#      run: |
#        ansible -m ping -i ./ansible_production/inventory all
  #
  #  - name: Exécuter le playbook Ansible
  #    run: |
  #      ls ../preProd-terraform/
  #      ansible-playbook -i inventory playbook.yml 
  #    working-directory: ./ansible/
#
  #  - name: Désactiver l'accès SSH depuis n'importe où
  #    run: |
  #      aws ec2 revoke-security-group-ingress --group-name admin_ssh_preprod \
  #        --protocol tcp --port 22 --cidr 0.0.0.0/0 --region us-east-1




# Explications
#    Artefact pour la clé SSH :
#    
#    L'étape actions/upload-artifact@v4 permet de sauvegarder la clé privée SSH (vockey.pem) générée par Terraform en tant qu'artefact.
#    
#    Cet artefact sera accessible depuis l'interface GitHub dans la section Actions après l'exécution du workflow.
#    
#    Téléchargement manuel de l'artefact :
#    
#    Une fois le workflow terminé, allez dans l'onglet Actions du repository GitHub.
#    
#    Sélectionnez le workflow correspondant.
#    
#    Téléchargez l'artefact nommé ssh-key.

# ou utiliser la commande bash sur la machine locale:
  # gh run download -R L-Christ-ASD/projet_final -n vockey.pem


# Étape 14 : Ajouter l'hôte EC2 aux hôtes connus
# - name: Ajouter l'hôte aux known_hosts
#   run: ssh-keyscan -H $(terraform output -raw public_ip) >> ~/.ssh/known_hosts

#Configuration des Secrets GitHub

#Ajoute ces secrets dans les paramètres de ton repo GitHub → Settings → Secrets → Actions :
#AWS_ACCESS_KEY_ID
#AWS_SECRET_ACCESS_KEY


# Explication du Workflow

#Clone le repo
#Installe Terraform et applique la config
#Récupère l'IP de l'instance AWS créée
#Ajoute la clé SSH pour la connexion
#Installe Ansible et exécute le playbook sur l'instance



# -===============producttion==========================

    

    - name: Initialiser Terraform pour le cluster rke2
      run: terraform init
      working-directory: ./production_tf/cluster_ec2/

    - name: Valider la config Terraform pour rke2
      run: terraform validate
      working-directory: ./production_tf/cluster_ec2/

    - name: Plan Terraform
      run: terraform plan
      working-directory: ./production_tf/cluster_ec2/

    # - name: Netoyer avant_apply (role, policy, instanceprofile) 
    #   run: |
    #     chmod +x cleanup_before_apply.sh
    #     ./cleanup_before_apply.sh
    #  working-directory: ./production_tf/cluster_ec2/

    - name: Lancer Terraform apply
      run: terraform apply -auto-approve
      working-directory: ./production_tf/cluster_ec2/

    - name: Sauvegarder le fichier "terraform.tfstate"
      uses: actions/upload-artifact@v4
      with:
        name: terraform-state
        path: ./production_tf/cluster_ec2/terraform.tfstate

    - name: Sauvegarder la clé SSH
      uses: actions/upload-artifact@v4
      with:
        name: vockeyprod.pem
        path: ./production_tf/cluster_ec2/vockeyprod.pem

    - name: Contenu du dossier ansible_production
      run: |
        ls ./ansible_production/

    - name: Contenu du fichier inventory
      run: |
        cat ./ansible_production/inventory

    - name: Autoriser SSH depuis n'importe où (temporaire)
      run: |
        aws ec2 authorize-security-group-ingress --group-name admin_ssh_production \
          --protocol tcp --port 22 --cidr 0.0.0.0/0 --region us-east-1

    - name: Contenu du dossier terraform cluster_ec2 et arbo
      run: |
        pwd
        ls ./production_tf/cluster_ec2/
        ls -l production_tf/cluster_ec2/vockeyprod.pem
        ls -R

    - name: Chmod 400 sur .pem
      run: |
        chmod 600 ./production_tf/cluster_ec2/vockeyprod.pem
    
    - name: Vérifier les permissions de la clé .pem
      run: |
        ls -l ./production_tf/cluster_ec2/vockeyprod.pem

    - name: Attendre le demarrage complet des instances 60s
      run: |
        sleep 60s

    # Vérifier la connexion SSH avec Ansible (ping sur toutes les machines)
    - name: Copier la clé SSH dans ./ pour le ping workflow
      run: |
        cp ./production_tf/cluster_ec2/vockeyprod.pem ./
        chmod 600 vockeyprod.pem

    - name: Copier la clé SSH dans ansible_production (Pour le fichier inventory)
      run: |
        cp ./production_tf/cluster_ec2/vockeyprod.pem ./ansible_production/
        chmod 600 ./ansible_production/vockeyprod.pem

    #- name: Copier apotheose_rke2_manifests dans ansible_production/
    #  run: |
    #    cp -r ./apotheose_rke2_manifests ./ansible_production/

    - name: Test SSH connection with Ansible
      run: |
        ansible -m ping -i ./ansible_production/inventory all


# ============================= cluster rke2 ================================
    - name:  Déploiement du cluster rke2 !!
      run: |
        echo "Configuration et mise en place du (playbook Ansible_prod)"

        ansible-playbook -i inventory playbook.yml 
      working-directory: ./ansible_production/

    - name: Cluster RKE2 déployé avec succès!! Bravo Christ !!
      run: |
        sleep 2s

    
#================ Déploiement du projet apotheose (CD)==================================

# ==========Copier le fichier kubeconfig depuis master1=================
    - name: Extraire l'IP de master1 depuis l'inventaire (copier rke2.yml et changer l'ip)
      run: |
        MASTER1_IP=$(awk '/^master1/ { for(i=1;i<=NF;i++) if($i ~ /ansible_host=/) { split($i,a,"="); print a[2] } }' ./ansible_production/inventory)
        echo "MASTER1_IP=$MASTER1_IP" >> $GITHUB_ENV
        echo "MASTER1_IP=$MASTER1_IP"

    - name: creer le dossier .kube
      run: |
        mkdir -p .kube

    - name: Copier le fichier kubeconfig depuis master1
      run: |
        scp -i vockeyprod.pem -o StrictHostKeyChecking=no ubuntu@$MASTER1_IP:/etc/rancher/rke2/rke2.yaml .kube/config-rke2

    - name: Remplacer 127.0.0.1 par l'IP publique dans kubeconfig.yaml
      run: |
        sed -i "s/127.0.0.1/$MASTER1_IP/g" .kube/config-rke2

    - name: Vérifier .kube/ et le changement d'ip dans kubeconfig.yaml
      run: |
        ls -l .kube/
        cat .kube/config-rke2

    - name: Téléchrager KUBCTL et Vérifier l'URL de la dernière version stable de Kubernetes
      run: |
        VERSION=$(curl -sL https://dl.k8s.io/release/stable.txt)
        echo "URL de la version stable de Kubernetes : https://dl.k8s.io/release/$VERSION/bin/linux/amd64/kubectl"
        curl -LO "https://dl.k8s.io/release/$VERSION/bin/linux/amd64/kubectl"
    
    - name: Rendre kubectl exécutable et le déplacer
      run: |
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
      
    - name: kubectl version 
      run: |
        kubectl version --client

    - name: Configurer kubeconfig pour interagir avec le cluster
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl config view


# ============================= Manifests ================================        
    - name: Ouvrir le port 6443 (temporaire) pour le deploiement des manifest
      run: |
        aws ec2 authorize-security-group-ingress --group-name admin_ssh_production \
          --protocol tcp --port 6443 --cidr 0.0.0.0/0 --region us-east-1



    - name: Labeliser les masters et Patcher les nodes avec k8s
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl label node master1 node-role.kubernetes.io/control-plane= --overwrite
        kubectl label node master2 node-role.kubernetes.io/control-plane= --overwrite
        kubectl label node master3 node-role.kubernetes.io/control-plane= --overwrite
        
        echo "Patcher les nodes avec k8s"
        chmod +x ./aws_cloud/patch_nodes.sh
        ./aws_cloud/patch_nodes.sh

    - name: Appliquer la configmap aws-cloud-provider pour aws-ccm
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl apply -f ./aws_cloud/aws-cloud-provider-configmap.yaml

    #================================================================
# Préparer son disque /dev/xvdbf afin qu’il soit détecté correctement par OpenEBS

    # - name: Get NDM pod names
    #   id: get-ndm
    #   run: |
    #     export KUBECONFIG=.kube/config-rke2
    #     pods=$(kubectl get pods -n openebs -l app=openebs-ndm -o jsonpath='{.items[*].metadata.name}')
    #     echo "ndm_pods=$pods" >> $GITHUB_ENV
    #     echo "NDM pods: $pods"

    # - name: Delete each NDM pod to force rescan
    #   i f: env.ndm_pods != '''
    #   run: |
    #     export KUBECONFIG=.kube/config-rke2
    #     for pod in ${{ env.ndm_pods }; do
    #       echo "Deleting pod: $pod"
    #       kubectl delete pod -n openebs "$pod"
    #     done

    # - name: Wait for NDM pods to restart
    #   run: |
    #     export KUBECONFIG=.kube/config-rke2
    #     echo "Waiting for NDM pods to be Ready..."
    #     kubectl wait --for=condition=Ready pod -n openebs -l app=openebs-ndm --timeout=90s

    # - name: List blockdevices
    #   run: |
    #     export KUBECONFIG=.kube/config-rke2
    #     echo "Listing blockdevices:"
    #     kubectl get blockdevices -n openebs -o custom-columns="NAME:.metadata.name,NODE:.spec.nodeAttributes.hostname"
        


# =========================== OpenEBS (cluster) avec cStor ================================================
    
    # Deploiement avec helm (chaque service) 

    - name: Installer Helm !!
      run: |
        curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
        chmod 700 get_helm.sh
        ./get_helm.sh

    - name: Vérifier la version de Helm!!
      run: |
        helm version
    
        

    # OpenEBS


        
    # echo "Reclamer les bd (55Gi/60) pour openebs et les activer"
    # chmod +x ./OpenEBScStorPoolCluster/blockdevice_claims.sh
    # ./OpenEBScStorPoolCluster/blockdevice_claims.sh


    - name: Installer et configurer OpenEBS (cluster) repo officiel
      run: |
        export KUBECONFIG=.kube/config-rke2

        echo "Labeliser le node workers --> node-role.kubernetes"
        kubectl label node worker1 node-role.kubernetes.io/worker=true
        kubectl label node worker2 node-role.kubernetes.io/worker=true
        
        
        echo "Ajouter le repo openebs (officiel)"
        helm repo add openebs https://openebs.github.io/charts
        helm repo update
        
        echo "Modifier les annotations pour eviter les conflits"
        chmod +x ./OpenEBScStorPoolCluster/annotations_CRDs_OpenEBS.sh
        ./OpenEBScStorPoolCluster/annotations_CRDs_OpenEBS.sh


        echo "Déploiement du repo officiel de helm, avec values.yaml"
        echo "... uniquement sur les workers"
        helm upgrade --install openebs openebs/openebs \
        --namespace openebs \
        --create-namespace \
        --set cstor.enabled=true \
        --set crds.enabled=false \
        --values ./OpenEBScStorPoolCluster/values.yaml \
        --reuse-values 

        echo "Attendre que les pods soient prêts"
        chmod +x ./OpenEBScStorPoolCluster/wait_for_pods_ready.sh
        ./OpenEBScStorPoolCluster/wait_for_pods_ready.sh

        
        sleep 10
        echo "Attendre que les BlockDevices des workers soient détectés"
        chmod +x ./OpenEBScStorPoolCluster/wait_for_detecting_workers_bd.sh
        ./OpenEBScStorPoolCluster/wait_for_detecting_workers_bd.sh       

        echo "Voir les blockdevices détectés:"
        echo "Block-devices disponibles :"
        kubectl get blockdevices -n openebs -o wide || echo "Aucun block trouvé"


        
        echo "Attendre que tous les BlockDevices des workers passent à l'état 'Active"
        chmod +x ./OpenEBScStorPoolCluster/wait_for_blockdevices_active.sh
        ./OpenEBScStorPoolCluster/wait_for_blockdevices_active.sh
         
        echo "Voir le STATUS des Block-devices disponibles des workers:"
        kubectl get blockdevices -n openebs -o wide || echo "Aucun block trouvé"

        echo "Création et application automatique du fichier 'cstor-pool.yaml' 'CStorPoolCluster'"
        chmod +x ./OpenEBScStorPoolCluster/création_des_pools_cStor.sh
        ./OpenEBScStorPoolCluster/création_des_pools_cStor.sh

        echo "Vérifier la création du fichier 'cstor-pool.yaml'"
        cat ./cstor-pool.yaml

        echo "Vérifier les labels"
        kubectl get node --show-labels

        
        echo "Vérifier si les bd sont "clamed" avant de déployer apotheose"
        echo "Block-devices détectés :"
        kubectl get blockdevices -n openebs -o wide
        
        echo "Vérifier le cStorpoolcluster :"
        kubectl get cstorpoolcluster -n openebs || echo "Aucun cstorpoolcluster trouvé pour l'instant"

        echo "cstor-csi détectés :"
        kubectl get pods -n openebs | grep cstor-csi || echo "Aucun pod cstor-csi trouvé (encore en cours de déploiement ?)"
        

        echo "Appliquer le storageclass cStor"
        kubectl apply -f ./OpenEBScStorPoolCluster/StorageClasscStor.yaml

        echo "Voir le STATUS des Block-devices disponibles des workers:"
        kubectl get blockdevices -n openebs -o wide || echo "Aucun block trouvé"


    - name: Vérifier que le ServiceAccount cloud-controller-manager a bien été créé dans le namespace kube-system
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl get serviceaccounts -n kube-system

        echo "Vérifier si aws-CCM est actif '(aws)-cloud-controller-manager-xxxxx'"
        kubectl get pods -n kube-system

        
      # echo "Mettre OpenEBS à jour avec cStorPoolCluster (CSPC)"
      # kubectl apply -f ./OpenEBScStorPoolCluster/
  # echo "Mettre OpenEBS à jour avec ma custumconf helm"
  # helm upgrade --install openebs ./OpenEBS_custum -f ./OpenEBS_custum/values.yaml --namespace openebs --create-namespace


    - name: Vérifier "eipAllocationId" dans /helm_apotheose/traefik/values.yaml
      run: |
        cat ./helm_apotheose/traefik/values.yaml
        


# ============= Deployer apotheose - Don't Repeat Yourself =======================================

    # Pour [INFO] Chart.yaml: icon is recommended ==> ajouter la ligne ci-dessous dans chaque Chart.yaml
    # icon: https://raw.githubusercontent.com/helm/chartmuseum/master/logo.pn

    # Pour valider un template localement:
    # helm template traefik ./helm_apotheose/traefik -f ./helm_apotheose/traefik/values.yaml

    - name: Lint de tous les charts Helm du projet apotheose
      run: |
        for chart in ./helm_apotheose/*; do
          if [ -d "$chart" ]; then
            echo "Lint de $chart..."
            helm lint "$chart"
          fi
        done


    - name: Ajouter le repo Helm Traefik (officiel)
      run: helm repo add traefik https://helm.traefik.io/traefik && helm repo update

    - name: Déploiement apotheose avec Helm Charts 
      run: |
        export KUBECONFIG=.kube/config-rke2

        echo "Déploiement de traefik...( Chart officiel)"
        helm upgrade --install traefik traefik/traefik --namespace apotheose --create-namespace

    - name: Supprimer le "deployment" officiel pour déployer traefik ( custum Chart )
      run: |
        export KUBECONFIG=.kube/config-rke2

        echo "Delete (officiel) deployment traefik"
        kubectl delete deployment traefik -n apotheose || echo "deployment Traefik (officiel) déjà supprimé"

        echo "Donner le ns apotheose à helm pour traefik (au cas où !)"
        kubectl label namespace apotheose app.kubernetes.io/managed-by=Helm
        kubectl annotate namespace apotheose meta.helm.sh/release-name=traefik
        kubectl annotate namespace apotheose meta.helm.sh/release-namespace=apotheose

      # echo "Appliquer le volume PVC Traefik letsencrypt (custum-manifests) avant deploiement" 
      # kubectl apply -f ./k8s-manifests/pv-traefik.yaml
      #--------------------------------------------------------------------------
        # echo "Redémarrer le déploiement d'OpenEBS"
        # kubectl rollout restart deployment openebs-cstor-admission-server -n openebs

        # echo "Attendre 20s"
        # sleep 20s

        # echo "Vérifier que les pods OPENEBS sont en mode Running"
        # kubectl get pods -n openebs
        # echo "Vérifier les svc dans le ns OPENEBS"
        # kubectl get svc -n openebs



    # accorder tous les droits sur toutes les ressources dans le namespace apotheose pour dignostique!
    - name: Appliquer et bind les roles k8s ( full acces pour les logs)
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl apply -f ./rke2_Roles/

        echo "Vérifier les pods OPENEBS"
        kubectl get pods -n openebs
        kubectl get svc -n openebs

        echo "Déploiement de Traefik (custum-chart)"
        helm upgrade --install traefik ./helm_apotheose/traefik/ -f ./helm_apotheose/traefik/values.yaml --namespace apotheose --create-namespace

        echo "attendre 30s"
        sleep 30s

    - name: Lister le svc traefik  déployés dans apotheose 
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl get svc -n apotheose traefik -o wide

        echo "Vérifier que la StorageClass est bien créée"
        kubectl get storageclass 

  
    - name: Voir l’IP publique de Traefik (EIP)
      run: |
        export KUBECONFIG=.kube/config-rke2
        aws ec2 describe-addresses

        echo "Service Traefik exposé avec l’EIP :"
        kubectl get svc traefik -n apotheose -o=custom-columns=NAME:.metadata.name,EXTERNAL-IP:.status.loadBalancer.ingress[*].ip,PORTS:.spec.ports[*].port
    

    - name: Déployer les autres charts
      run: |
        export KUBECONFIG=.kube/config-rke2

        echo "Donner le ns apotheose à helm"
        kubectl label namespace apotheose app.kubernetes.io/managed-by=Helm --overwrite
        kubectl annotate namespace apotheose meta.helm.sh/release-namespace=apotheose --overwrite

        echo "Supprimer les webhooks NGINX de RKE2 pour traefik"
        kubectl delete validatingwebhookconfiguration rke2-ingress-nginx-admission

        echo "Vérifier les pods dans openebs"
        kubectl get pods -n openebs

        echo "Attendre que les webhook d'admission OPENEBS soit prets ou redemarer les pods"
        chmod +x ./OpenEBScStorPoolCluster/wait_for_admision_pods.sh
        ./OpenEBScStorPoolCluster/wait_for_admision_pods.sh

        for chart in ./helm_apotheose/*; do
          name=$(basename "$chart")
          if [[ -d "$chart" && "$name" != "traefik" ]]; then
            echo "Déploiement de $name..."
            helm upgrade --install "$name" "$chart" \
              --namespace apotheose \
              --values "$chart/values.yaml"
          fi
        done


    - name: pause de 60s
      run: |
        sleep 60s
    
    - name: Vérifier l'état du cluster
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl get nodes
        kubectl get pods -n apotheose
        

    - name: Lister tous les pods dans le namespace apotheose
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl get pods -n apotheose -o wide

    - name: Lister services_apotheose
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl get svc -n apotheose

    - name: Lister Ingress déployés_apotheose
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl get ing -n apotheose

    - name: Lister  les statuts des PVC et pods déployés_apotheose
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl get pvc -n apotheose
        

    - name: Apothéos déployé avec succès!! Bravo Christ !!
      run: |
        sleep 30s

    - name: En fin, lister les PVC, pods et svc déployés dans le ns apotheose !
      run: |
        export KUBECONFIG=.kube/config-rke2
        kubectl get pvc -n apotheose
        kubectl get pods -n apotheose -o wide
        kubectl get svc -n apotheose
        kubectl get svc -n apotheose traefik -o wide

        echo "Afficher Values de treafik"
        helm get values traefik -n apotheose

        echo "Afficher les annotations du svc traefik"
        kubectl get svc traefik -n apotheose -o yaml | grep -A 10 annotations



# ================ OpenVPN BASTION ==================

#    - name: Générer une clé SSH
#      run: ssh-keygen -t rsa -b 4096 -f ~/.ssh/id_rsa -q -N ""
#
#    - name: Copier la clé publique vers le serveur
#      run: ssh-copy-id -i ~/.ssh/id_rsa.pub user@172.11.64.20
#
#    - name: Copier la clé publique vers la machine locale
#      run: ssh-copy-id -i ~/.ssh/id_rsa.pub user@127.0.0.1
#
#    - name: Configuration des clés SSH
#      run: |
#        echo "${{ secrets.SSH_CHRIST }" > ~/.ssh/id_rsa
#        chmod 600 ~/.ssh/id_rsa
#




#======================  Attendre l'attribution d'une EXTERNAL-IP =======================
    # - name: Attendre que le LoadBalancer Traefik ait une EXTERNAL-IP
    #   run: |
    #       export KUBECONFIG=.kube/config-rke2

    #       SERVICE_NAME="traefik"
    #       NAMESPACE="apotheose"
    #       TIMEOUT=300  # secondes
    #       SLEEP=5
    #       ELAPSED=0

    #       echo "Attente de l'EXTERNAL-IP du service $SERVICE_NAME..."

    #       while true; do
    #         EXTERNAL_IP=$(kubectl get svc $SERVICE_NAME -n $NAMESPACE -o jsonpath="{.status.loadBalancer.ingress[0].hostname}")
    #         if [[ -n "$EXTERNAL_IP" ]]; then
    #           echo "EXTERNAL-IP trouvée : $EXTERNAL_IP"
    #           break
    #         fi

    #         if [[ $ELAPSED -ge $TIMEOUT ]]; then
    #           echo "⛔ Temps d'attente dépassé ($TIMEOUT s), EXTERNAL-IP non assignée"
    #           exit 1
    #         fi

    #         echo "⏳ En attente... ($ELAPSED / $TIMEOUT)"
    #         sleep $SLEEP
    #         ELAPSED=$((ELAPSED + SLEEP))
    #       done

    #       shell: bash




    - name: Fermerl'accès au port 6443!!
      run: |
        aws ec2 revoke-security-group-ingress --group-name admin_ssh_production \
          --protocol tcp --port 6443 --cidr 0.0.0.0/0 --region us-east-1

    - name: Oups, J'allais oublier...Désactivation de l'accès SSH depuis n'importe où !!
      run: |
        aws ec2 revoke-security-group-ingress --group-name admin_ssh_production \
          --protocol tcp --port 22 --cidr 0.0.0.0/0 --region us-east-1


      